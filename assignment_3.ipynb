{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JerryHung1103/ML/blob/PA3/assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9RfhUxKJCyy"
      },
      "source": [
        "# Q1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgpbAVOuJCy0"
      },
      "source": [
        "## Code for (1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4F9M6pI9JCy0"
      },
      "outputs": [],
      "source": [
        "# Setup environment\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# Build MLP\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        #######################\n",
        "        # TODO: initialize neural network components\n",
        "        self.fc1 = nn.Linear(784, 350)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(350, 150)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(150, 10)\n",
        "\n",
        "\n",
        "\n",
        "        #######################\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        #######################\n",
        "        # TODO: define forwarding of MLP\n",
        "        x = x.view(-1, 784)\n",
        "\n",
        "        # Define forwarding of MLP\n",
        "        x = self.relu1(self.fc1(x))\n",
        "        x = self.relu2(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #######################\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "  mlp = MLP()\n",
        "  print(f'The MLP structure you built is as follow: \\n{mlp}')"
      ],
      "metadata": {
        "id": "CN66jq-zXrGc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cb6e2e6-ac54-48e4-b491-b1f8db10b74f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The MLP structure you built is as follow: \n",
            "MLP(\n",
            "  (fc1): Linear(in_features=784, out_features=350, bias=True)\n",
            "  (fc2): Linear(in_features=350, out_features=150, bias=True)\n",
            "  (fc3): Linear(in_features=150, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code for (2)\n",
        "Note: run this code after code for (1)"
      ],
      "metadata": {
        "id": "Qfcew0bsJPeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate number of parameters\n",
        "def count_parameters(model):\n",
        "    param_dict = {}\n",
        "    #######################\n",
        "    # TODO: Iterate the model parameters and their names, and save them into dictionary param_dict with the following format -- name: number of param\n",
        "    for name, param in model.named_parameters():\n",
        "        num_param = param.numel()\n",
        "        param_dict[name] = num_param\n",
        "\n",
        "\n",
        "    #######################\n",
        "    return param_dict"
      ],
      "metadata": {
        "id": "TiDq03E1JOFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "  mlp = MLP()\n",
        "  print(count_parameters(mlp))"
      ],
      "metadata": {
        "id": "vSWAIkwbX6o5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bd078ef-2a7b-4915-b500-fe7cde057285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fc1.weight': 274400, 'fc1.bias': 350, 'fc2.weight': 52500, 'fc2.bias': 150, 'fc3.weight': 1500, 'fc3.bias': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUklizTZJCy0"
      },
      "source": [
        "## Code for (3)\n",
        "Note: run this code after code for (1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_gjkfFfJCy1"
      },
      "outputs": [],
      "source": [
        "# Setup environment\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "def train(model):\n",
        "  torch.manual_seed(1)\n",
        "\n",
        "  # Prepare dataset\n",
        "  train_set = torchvision.datasets.MNIST(\"data/\", train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
        "  test_set = torchvision.datasets.MNIST(\"data/\", train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
        "\n",
        "  #######################\n",
        "  # TODO: define train_loader, test_loader using train_set, test_set, the utils.data.DataLoader function\n",
        "  import torch.utils.data as data\n",
        "  train_loader = data.DataLoader(train_set, batch_size=25)\n",
        "  test_loader = data.DataLoader(test_set, batch_size=1000)\n",
        "  #######################\n",
        "\n",
        "  # Build MLP\n",
        "  mlp = model\n",
        "\n",
        "  # Train MLP\n",
        "  lossFunc = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(mlp.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "  #######################\n",
        "  # TODO: Train the MLP for 3 epochs\n",
        "  for epoch in range(3):\n",
        "        running_loss = 0.0\n",
        "        for i, (inputs, labels) in enumerate(train_loader, 0):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = mlp(inputs)\n",
        "            loss = lossFunc(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "\n",
        "\n",
        "  #######################"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model):\n",
        "    # toggle evaluation mode\n",
        "    model.eval()\n",
        "    results = {}\n",
        "\n",
        "    # Prepare dataset\n",
        "    train_set = torchvision.datasets.MNIST(\"data/\", train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
        "    test_set = torchvision.datasets.MNIST(\"data/\", train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
        "\n",
        "    #######################\n",
        "    # TODO: define train_loader, test_loader using train_set, test_set, the utils.data.DataLoader function\n",
        "    import torch.utils.data as data\n",
        "    train_loader = data.DataLoader(train_set, batch_size=25)\n",
        "    test_loader = data.DataLoader(test_set, batch_size=1000)\n",
        "    #######################\n",
        "\n",
        "    # define an evaluation function\n",
        "    def evaluate_a_model(model, dataloader):\n",
        "        correct = 0\n",
        "        with torch.no_grad():\n",
        "            #######################\n",
        "            # TODO: Calculate the number of correctly predicted samples and store the number into variable \"correct\"\n",
        "            for data, target in dataloader:\n",
        "              output = model(data)\n",
        "              pred = output.argmax(dim=1, keepdim=True)\n",
        "              correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "            #######################\n",
        "        accuracy = torch.tensor(100.*correct / len(dataloader.dataset))\n",
        "        return accuracy\n",
        "\n",
        "    # evaluate on training data\n",
        "    train_acc = evaluate_a_model(model, train_loader)\n",
        "    # evaluate on testing data\n",
        "    test_acc = evaluate_a_model(model, test_loader)\n",
        "    results['Final training accuracy'] = round(train_acc.item(), 2)\n",
        "    results['Final testing accuracy'] = round(test_acc.item(), 2)\n",
        "    return results"
      ],
      "metadata": {
        "id": "YZ_E1umWX_b9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "    mlp = MLP()\n",
        "    train(mlp)\n",
        "    results = evaluate(mlp)\n",
        "    print(results)"
      ],
      "metadata": {
        "id": "ElGF0MBMYPYU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d656e818-2cbb-49c6-a534-db6ac1dfaad6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Final training accuracy': 97.3, 'Final testing accuracy': 96.38}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-KVP9P-YzCP"
      },
      "source": [
        "# Q2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code for (1)"
      ],
      "metadata": {
        "id": "Tn2Xx2ObtHDK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHZnPL63YzCa"
      },
      "outputs": [],
      "source": [
        "# Setup environment\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# Build Convolutional Neural Network (CNN)\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        #######################\n",
        "        # TODO: initialize neural network components\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "\n",
        "\n",
        "        #######################\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        #######################\n",
        "        # TODO: define forwarding for CNN\n",
        "         # Define forwarding for CNN\n",
        "        x = self.relu1(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = self.relu2(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "\n",
        "        #######################\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "  cnn = CNN()\n",
        "  print(f'The CNN structure you built is as follow: \\n{cnn}')"
      ],
      "metadata": {
        "id": "N5kRJ0LVtscd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f76c22b-5fa4-422c-9f74-0f27bc20e770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The CNN structure you built is as follow: \n",
            "CNN(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (relu1): ReLU()\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (relu2): ReLU()\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
            "  (relu3): ReLU()\n",
            "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code for (2)"
      ],
      "metadata": {
        "id": "61z4Y9BwzInm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_total_parameters(model):\n",
        "    total_params = 0\n",
        "    #######################\n",
        "    # TODO: Iterate the parameters in CNN; Save the number of learnable param, and add it to total_params\n",
        "    for param in model.parameters():\n",
        "      if param.requires_grad:\n",
        "        total_params += param.numel()\n",
        "\n",
        "    #######################\n",
        "    return total_params"
      ],
      "metadata": {
        "id": "UQwfXdv-zK6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "  cnn = CNN()\n",
        "  total_params = count_total_parameters(cnn)\n",
        "  print(f'Total number of learnable parameters: {total_params}')"
      ],
      "metadata": {
        "id": "9ho6PqB8zLe8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91234cf6-2aad-432e-9552-5d6e919984c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of learnable parameters: 421642\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLJphw5tJCy1"
      },
      "source": [
        "## Code for (3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVc1AaWkJCy1"
      },
      "outputs": [],
      "source": [
        "# Setup environment\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "def train_cnn(model):\n",
        "  torch.manual_seed(1)\n",
        "\n",
        "  # Prepare dataset\n",
        "  train_set = torchvision.datasets.MNIST(\"data/\", train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
        "  test_set = torchvision.datasets.MNIST(\"data/\", train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
        "\n",
        "  #######################\n",
        "  # TODO: define the train_loader, test_loader using train_set, test_set, the utils.data.DataLoader function\n",
        "  import torch.utils.data as data\n",
        "  train_loader = data.DataLoader(train_set, batch_size=200)\n",
        "  test_loader = data.DataLoader(test_set, batch_size=1000)\n",
        "  #######################\n",
        "\n",
        "\n",
        "  # Build Convolutional Neural Network (CNN)\n",
        "  cnn = model\n",
        "\n",
        "  # Train CNN\n",
        "  lossFunc = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(cnn.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "  #######################\n",
        "  # TODO: Train the CNN for 3 epochs\n",
        "\n",
        "  for epoch in range(3):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = cnn(inputs)\n",
        "            loss = lossFunc(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  return cnn\n",
        "  #######################"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_cnn(model):\n",
        "    # toggle evaluation mode\n",
        "    model.eval()\n",
        "    results = {}\n",
        "\n",
        "    # Prepare dataset\n",
        "    train_set = torchvision.datasets.MNIST(\"data/\", train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
        "    test_set = torchvision.datasets.MNIST(\"data/\", train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
        "\n",
        "    #######################\n",
        "    # TODO: define train_loader, test_loader using train_set, test_set, the utils.data.DataLoader function\n",
        "    import torch.utils.data as data\n",
        "    train_loader = data.DataLoader(train_set, batch_size=200)\n",
        "    test_loader = data.DataLoader(test_set, batch_size=1000)\n",
        "    #######################\n",
        "\n",
        "    # define an evaluation function\n",
        "    def evaluate_a_model(model, dataloader):\n",
        "        correct = 0\n",
        "        with torch.no_grad():\n",
        "            #######################\n",
        "            # TODO: Calculate the number of correctly predicted samples and store the number into variable \"correct\"\n",
        "            for data in dataloader:\n",
        "                inputs, labels = data\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "            #######################\n",
        "        accuracy = torch.tensor(100.*correct / len(dataloader.dataset))\n",
        "        return accuracy\n",
        "\n",
        "    # evaluate on training data\n",
        "    train_acc = evaluate_a_model(model, train_loader)\n",
        "    # evaluate on testing data\n",
        "    test_acc = evaluate_a_model(model, test_loader)\n",
        "    results['Final training accuracy'] = round(train_acc.item(), 2)\n",
        "    results['Final testing accuracy'] = round(test_acc.item(), 2)\n",
        "    return results"
      ],
      "metadata": {
        "id": "Tc8dGuM-1HTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "    cnn = CNN()\n",
        "    train_cnn(cnn)\n",
        "    results = evaluate_cnn(cnn)\n",
        "    print(results)"
      ],
      "metadata": {
        "id": "u5OCy0GzMrEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3"
      ],
      "metadata": {
        "id": "49_-9fKFPMZ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare the dataset"
      ],
      "metadata": {
        "id": "JJcN9oO3Lhrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import sklearn"
      ],
      "metadata": {
        "id": "zx4XskusYzMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read Dataset\n",
        "Now you have the needed libraries in hand. Next, let's read the dataset from the source file to the project.  \n",
        "\n",
        "We assume you are working in Google Colab. One way to read a dataset in Google Colab:\n",
        "1. Download the source file and put it on your Google Drive\n",
        "2. Import the `drive` module from `google.colab` package\n",
        "3. Run `drive.mount` to mount your Google Drive to the Colab notebook\n",
        "4. Use `pandas.read_csv` to read the data from Google Drive and store the data in pandas DataFrame\n",
        "\n",
        "Todo:\n",
        "Modify `YourFilePath` depending on the actual directory to read the data to this notebook.\n",
        "\n",
        "Remarks:  \n",
        "You can check whether your data reading is successful by running the next cell. The shape should be (4892, 12). You can also see the first 5 rows of the dataset."
      ],
      "metadata": {
        "id": "bSnurZXzqbt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "# todo start #\n",
        "# please modify YourFilePath\n",
        "    data_file = \"/content/drive/MyDrive/xxx/heart_failure_clinical_records_dataset.csv\"\n",
        "    # data_df = pd.read_csv(data_file)\n",
        "    # data_df.insert(0, 'patient_id', range(1, 1 + len(data_df)))\n",
        "    data_df = pd.read_csv(data_file)\n",
        "    data_df.insert(0, 'patient_id', range(1, 1 + len(data_df)))\n",
        "# todo end #\n",
        "    print(data_df.head(10))"
      ],
      "metadata": {
        "id": "1u0Nr4npbBbB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15a067d2-2eed-41f0-857a-370b0eec5694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "   patient_id   age  anaemia  creatinine_phosphokinase  diabetes  \\\n",
            "0           1  75.0        0                       582         0   \n",
            "1           2  55.0        0                      7861         0   \n",
            "2           3  65.0        0                       146         0   \n",
            "3           4  50.0        1                       111         0   \n",
            "4           5  65.0        1                       160         1   \n",
            "5           6  90.0        1                        47         0   \n",
            "6           7  75.0        1                       246         0   \n",
            "7           8  60.0        1                       315         1   \n",
            "8           9  65.0        0                       157         0   \n",
            "9          10  80.0        1                       123         0   \n",
            "\n",
            "   ejection_fraction  high_blood_pressure  platelets  serum_creatinine  \\\n",
            "0                 20                    1  265000.00               1.9   \n",
            "1                 38                    0  263358.03               1.1   \n",
            "2                 20                    0  162000.00               1.3   \n",
            "3                 20                    0  210000.00               1.9   \n",
            "4                 20                    0  327000.00               2.7   \n",
            "5                 40                    1  204000.00               2.1   \n",
            "6                 15                    0  127000.00               1.2   \n",
            "7                 60                    0  454000.00               1.1   \n",
            "8                 65                    0  263358.03               1.5   \n",
            "9                 35                    1  388000.00               9.4   \n",
            "\n",
            "   serum_sodium  sex  smoking  time  DEATH_EVENT  \n",
            "0           130    1        0     4            1  \n",
            "1           136    1        0     6            1  \n",
            "2           129    1        1     7            1  \n",
            "3           137    1        0     7            1  \n",
            "4           116    0        0     8            1  \n",
            "5           132    1        1     8            1  \n",
            "6           137    1        0    10            1  \n",
            "7           131    1        1    10            1  \n",
            "8           138    0        0    10            1  \n",
            "9           133    1        1    10            1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code for (1)"
      ],
      "metadata": {
        "id": "jmYQjiI-PPAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kmeans(X, n_clusters):\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    rng = np.random.RandomState(42)\n",
        "    random_idx = rng.permutation(X.shape[0])[:n_clusters]\n",
        "    ###########################################\n",
        "    # TODO: Step 1: Randomly initialize the centroids\n",
        "    centroids = X[random_idx]\n",
        "    ###########################################\n",
        "\n",
        "    while True:\n",
        "        ###########################################\n",
        "        # TODO: Step 2: Assign each data point to the closest centroid\n",
        "        # Hint: labels is a numpy array of shape (n_samples,), where n_samples is the number of samples in the input data X.\n",
        "        #       Each element in labels is an integer that represents the cluster index to which the corresponding sample in X is assigned.\n",
        "        #       The cluster indices range from 0 to n_clusters - 1.\n",
        "        distances = np.sqrt(((X - centroids[:, np.newaxis])**2).sum(axis=2))\n",
        "        labels = np.argmin(distances, axis=0)\n",
        "        ###########################################\n",
        "\n",
        "        ###########################################\n",
        "        # TODO: Step 3: Compute new centroids as the mean of the data points in each cluster\n",
        "        # Hint: new_centroids is a numpy array of shape (n_clusters, n_features), where n_clusters is the number of clusters and n_features\n",
        "        #       is the number of features in the input data X. Each row in new_centroids represents the coordinates of a centroid in the feature space.\n",
        "        #       The centroids are computed as the mean of the samples in X that are assigned to each cluster.\n",
        "        new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(n_clusters)])\n",
        "        ###########################################\n",
        "\n",
        "        ###########################################\n",
        "        # TODO: Step 4: Check for convergence (i.e., no change in the centroids)\n",
        "        if np.allclose(centroids, new_centroids):\n",
        "            break\n",
        "        centroids = new_centroids\n",
        "        ###########################################\n",
        "\n",
        "    for i in range(len(centroids)):\n",
        "      results['centroid_'+str(i)] = np.round(centroids[i], 3)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "xSSUIJHjQKGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__==\"__main__\":\n",
        "  # Load data\n",
        "  subset = data_df[['age', 'high_blood_pressure', 'serum_creatinine', 'serum_sodium', 'smoking']].values\n",
        "\n",
        "  # Run K-Means\n",
        "  results = kmeans(subset, 3)\n",
        "  print(results)"
      ],
      "metadata": {
        "id": "Y_b4rwrtSjq7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94f367c3-9e0b-4d74-dd9d-0918b6ed5546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'centroid_0': array([ 76.138,   0.45 ,   1.579, 136.375,   0.362]), 'centroid_1': array([ 61.035,   0.315,   1.421, 136.935,   0.298]), 'centroid_2': array([ 47.684,   0.316,   1.203, 136.432,   0.316])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code for (2) & (3)"
      ],
      "metadata": {
        "id": "dajSdbSMUS4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def run_kmeans(X, n_clusters, init_method):\n",
        "    results = {}\n",
        "    ###########################################\n",
        "    # TODO: Initialize the KMeans object with the given number of clusters and initialization method\n",
        "    # Note: Set random_state parameter to 42 to ensure the output of function deterministic\n",
        "    kmeans = KMeans(n_clusters=n_clusters, init=init_method, random_state=42)\n",
        "    ###########################################\n",
        "\n",
        "    kmeans.fit(X)\n",
        "\n",
        "    ###########################################\n",
        "    # TODO: Retrieve the cluster centroids\n",
        "    centroids = kmeans.cluster_centers_\n",
        "    ###########################################\n",
        "\n",
        "    for i in range(len(centroids)):\n",
        "      results['centroid_'+str(i)] = np.round(centroids[i], 3)\n",
        "    return results"
      ],
      "metadata": {
        "id": "TX-vtOZfSuFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__==\"__main__\":\n",
        "  # Run K-Means with 'random' initialization\n",
        "  results = run_kmeans(subset, 3, 'random')\n",
        "  print(results)"
      ],
      "metadata": {
        "id": "XL6BcaONUVhC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a011d2c-3b0d-4405-84e1-99896ac67fb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'centroid_0': array([ 79.5  ,   0.442,   1.717, 136.038,   0.327]), 'centroid_1': array([ 48.858,   0.301,   1.192, 136.681,   0.31 ]), 'centroid_2': array([ 63.689,   0.358,   1.439, 136.806,   0.328])}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__==\"__main__\":\n",
        "  # Run K-Means with 'random' initialization\n",
        "  results = run_kmeans(subset, 3, 'k-means++')\n",
        "  print(results)"
      ],
      "metadata": {
        "id": "8GzHiLF8UbbS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86d229d3-9ab7-4e32-f051-2d54061852c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'centroid_0': array([ 79.5  ,   0.442,   1.717, 136.038,   0.327]), 'centroid_1': array([ 48.858,   0.301,   1.192, 136.681,   0.31 ]), 'centroid_2': array([ 63.689,   0.358,   1.439, 136.806,   0.328])}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "8ae69442b1fc44f14b5ed9e4e267de3f8165086ea0eeb0bf8dc28231d964179e"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}